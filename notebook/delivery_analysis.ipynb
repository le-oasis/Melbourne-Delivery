{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f9faae4-1e3e-48ba-a3cd-d6970db40272",
   "metadata": {
    "id": "2f9faae4-1e3e-48ba-a3cd-d6970db40272"
   },
   "source": [
    "## Step 0: Initiate Libraries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c231464-4a4b-4a66-8321-f96302dc09e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T14:45:29.892708Z",
     "start_time": "2023-07-04T14:45:29.731937Z"
    },
    "id": "0c231464-4a4b-4a66-8321-f96302dc09e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "# Import Key Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Import Data Preprocessing Libraries\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# ast : Abstract Syntax Trees\n",
    "from ast import literal_eval\n",
    "\n",
    "# Import Geospatial Libraries\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy.distance import geodesic\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import *\n",
    "import reverse_geocoder as rg \n",
    "\n",
    "# Datetime\n",
    "import datetime\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "import plotly.graph_objs as go\n",
    "from plotly.tools import FigureFactory as FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4266a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcc40e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34a3830a-cb7f-48ea-8e25-3c4aa759032e",
   "metadata": {
    "id": "34a3830a-cb7f-48ea-8e25-3c4aa759032e"
   },
   "source": [
    "## Step 1: Read Data\n",
    "\n",
    "- Here will we be reading the raw data as -  `dirty_data.csv` file into our jupyter notebook.\n",
    "- The variable name for the Food Delivery data would be called `dataset` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633453ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T14:45:29.903414Z",
     "start_time": "2023-07-04T14:45:29.893924Z"
    },
    "id": "633453ac",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the file path\n",
    "file_path = os.path.join(os.pardir, os.pardir, 'Melbourne-Delivery/data/dirty_data.csv')\n",
    "\n",
    "# Load the file into a DataFrame\n",
    "delivery_data = pd.read_csv(file_path)\n",
    "\n",
    "# Copy the data\n",
    "orders_df = delivery_data.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18a20a56-37c2-4dfb-8f66-28142e01931e",
   "metadata": {
    "id": "18a20a56-37c2-4dfb-8f66-28142e01931e"
   },
   "source": [
    "### Step 1.1: Data Discovery (Building Intuition)\n",
    "\n",
    "- This is a technique we use to get an initial feel for our data tables.\n",
    "- We read the data using pandas and perform method calls.\n",
    "- Standardize dataset columns in the correct format.\n",
    "- Explore Descriptive Statistics on Numerical Columns and more below:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63e102aa",
   "metadata": {
    "id": "63e102aa"
   },
   "source": [
    "##### `df.info()`\n",
    "\n",
    "- It is an important and widely used method of Python.\n",
    "- This Method prints the information or summary of the dataframe.\n",
    "- It prints the various information of the Dataframe such as index type, dtype, columns, non-values, and memory usage. It gives a quick overview of the dataset.\n",
    "- Info Method to get the Non-Null Count & Dtype (data type) of the dataset,\n",
    "- Validate if a column and column type aligns with the format of the Business Requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c083d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T14:45:29.955377Z",
     "start_time": "2023-07-04T14:45:29.922555Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c083d1a",
    "outputId": "2283d68b-902c-45d7-d0e1-f39f87c8390c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a0164e1",
   "metadata": {
    "id": "2a0164e1"
   },
   "source": [
    "Based on this information above:\n",
    "\n",
    "- The dataset has 500 rows and 12 columns.\n",
    "- The dataset has 0 null values.\n",
    "\n",
    "\n",
    "Data transformations and cleaning tasks that might be needed include:\n",
    "\n",
    "- Checking the data type of all columns to make sure they are in the appropriate format for analysis.\n",
    "- The `order_id`, `date`, `time`, `order_type`, `branch_code`, `order_items` columns datatypes need to be changed.\n",
    "- The `date` &  `time` need to be merged into a single column, renamed to `order_date` and converted to `datetime` dtype format for Parsing Dates.\n",
    "- Extracting the food items and their quantities from the `order_items` column into separate columns.\n",
    "- Checking the category column entries for case sensitivity (Checking if the values in the branch_code and order_type columns are consistent and correctly labeled.)\n",
    "- feature engineer the `order_date`  column, potentially separating them into year, month, day, and hour columns.\n",
    "- Removing any unnecessary or irrelevant columns.\n",
    "- Checking for outliers in numerical columns such as order_price, distance_to_customer_KM, and delivery_fee.\n",
    "- Checking if the customerHasloyalty? column only contains binary values (0 and 1).\n",
    "- Checking if the  `distance_to_customer_KM` column is consistent with the values in the `nodes.csv` file.\n",
    "- Checking if the `delivery_fee` column is consistent with the values in the `edges.csv`  file.\n",
    "- Checking if the `order_price` column is consistent with the values in the `order_items`  column.\n",
    "- Checking if the `order_price` column is consistent with the values in the  `delivery_fee` column.\n",
    "\n",
    "\n",
    "Remember, the goal of this process is to ensure that your dataset is clean, understandable, and ready for further Business Intelligence analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0458dc36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bFjYKlKRSj4-",
   "metadata": {
    "id": "bFjYKlKRSj4-"
   },
   "source": [
    "### Step 1.2: Data Preporcessing - Cleaning \n",
    "\n",
    "- Here we will be cleaning the data by converting the columns to the correct data types.\n",
    "- We will merge the date and time columns into one column called `order_date` and convert it to a datetime type.\n",
    "- We will also rename the `customerHasloyalty?` , `distance_to_customer_KM` columns to `customer_loyalty` , and  `distance_to_customer` respectively.\n",
    "- We will add an additional column called `updated at` which will be the date and time the data was updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eNo-KF7Sj1V",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T14:45:38.184964Z",
     "start_time": "2023-07-04T14:45:38.169814Z"
    },
    "id": "4eNo-KF7Sj1V",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reverseGeocode(coordinates): \n",
    "    result = rg.search(coordinates)\n",
    "    return (result)\n",
    "\n",
    "def cleaning_data_types(orders_df):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original one\n",
    "    df_clean = orders_df.copy()\n",
    "\n",
    "    # Define helper function to clean date data\n",
    "    def clean_date(date_str):\n",
    "        date_str = date_str.strip()\n",
    "        date = parse(date_str, dayfirst=True)  # dayfirst=True to handle DD/MM/YYYY properly\n",
    "        return date.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Convert columns to appropriate data types\n",
    "    df_clean['order_id'] = df_clean['order_id'].str.extract('(\\d+)').astype(int)\n",
    "    df_clean['date'] = df_clean['date'].apply(clean_date).astype('datetime64[ns]')\n",
    "    df_clean['datetime'] = pd.to_datetime(df_clean['date'].astype(str) + ' ' + df_clean['time'])\n",
    "    df_clean[\"order_type\"] = df_clean[\"order_type\"].astype(\"category\")\n",
    "\n",
    "   \n",
    "    # Convert 'branch_code' to upper case to handle case-insensitive duplicates\n",
    "    df_clean[\"branch_code\"] = df_clean[\"branch_code\"].str.upper().astype(\"category\")\n",
    "\n",
    "\n",
    "    # Use exception handling for potential errors in the literal_eval() function\n",
    "    try:\n",
    "        df_clean[\"order_items\"] = df_clean[\"order_items\"].apply(literal_eval)\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "\n",
    "    # Continue with the remaining conversions\n",
    "    df_clean[\"order_price\"] = df_clean[\"order_price\"].astype(float)\n",
    "    df_clean[\"customer_lat\"] = df_clean[\"customer_lat\"].astype(float)\n",
    "    df_clean[\"customer_lon\"] = df_clean[\"customer_lon\"].astype(float)\n",
    "    df_clean[\"customerHasloyalty?\"] = df_clean[\"customerHasloyalty?\"].astype(bool)\n",
    "    df_clean[\"distance_to_customer_KM\"] = df_clean[\"distance_to_customer_KM\"].astype(float)\n",
    "    df_clean[\"delivery_fee\"] = df_clean[\"delivery_fee\"].astype(float)\n",
    "\n",
    "\n",
    "    # make the order_price two decimal places\n",
    "    df_clean['order_price'] = df_clean['order_price'].round(2)\n",
    "\n",
    "    # make the delivery fee two decimal places\n",
    "    df_clean['delivery_fee'] = df_clean['delivery_fee'].round(2)\n",
    "\n",
    "\n",
    "    # transform long/lat into state\n",
    "    coordinates =list(zip(df_clean['customer_lat'],df_clean['customer_lon'])) # generates pair of (lat,long)\n",
    "    data = reverseGeocode(coordinates)\n",
    "\n",
    "\n",
    "    # Create a new column with the City name    \n",
    "    df_clean['name'] = [i['name'] for i in data]\n",
    "    df_clean['admin1'] = [i['admin1'] for i in data]\n",
    "    df_clean['admin2'] = [i['admin2'] for i in data]\n",
    "\n",
    "\n",
    "    df_clean.drop(['admin1', 'admin2'], axis=1, inplace=True)\n",
    "    df_clean.rename(columns={'name': 'location'}, inplace=True)\n",
    " \n",
    "\n",
    "    # Rename the customerHasloyalty? column to customerHasloyalty\n",
    "    df_clean.rename(columns={'customerHasloyalty?': 'customer_loyalty'}, inplace=True)\n",
    "\n",
    "    # Rename the distance_to_customer_KM column to distance_to_customer_km\n",
    "    df_clean.rename(columns={'distance_to_customer_KM': 'distance_to_customer_km'}, inplace=True)\n",
    "\n",
    "    # Drop the 'date' and 'time' columns\n",
    "    df_clean.drop(['date', 'time'], axis=1, inplace=True)\n",
    "\n",
    "    # Rename the 'datetime' column to 'order_date' and move it to the second position\n",
    "    df_clean.rename(columns={'datetime': 'order_date'}, inplace=True)\n",
    "    order_date = df_clean.pop('order_date')\n",
    "    df_clean.insert(1, 'order_date', order_date)\n",
    "\n",
    "    # Add the 'updated_at' column with the current datetime\n",
    "    df_clean['updated_at'] = datetime.datetime.today().replace(second=0, microsecond=0)\n",
    "\n",
    "    df_clean.drop(['customer_lat', 'customer_lon'], axis=1, inplace=True)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "df_clean = cleaning_data_types(orders_df)\n",
    "df_clean.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6f7de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f96806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('clean_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54fc64ae",
   "metadata": {},
   "source": [
    "### Step 1.3: Feature Engineering\n",
    "\n",
    "- Here we will be creating new columns from existing columns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1327cc2d",
   "metadata": {},
   "source": [
    "#### Step 1.4: Feature Engineering ( `order_items` )\n",
    "\n",
    "- Here we will be extracting the food items and their quantities from the `order_items` column into separate columns.\n",
    "- `cuisine` which will be the type of cuisine the food item is.\n",
    "-  `order_items_count` which will be the total number of items ordered.\n",
    "-  `order_items_total` which will be the total price of the items ordered."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16a06931",
   "metadata": {},
   "source": [
    "#### Step 1.5: Feature Engineering ( `order_date` )\n",
    "\n",
    "- Here we will be creating new columns based on the existing columns in the dataset.\n",
    "- `order_time_of_day` which will be the hour of the day the order was made. (Morning, Afternoon, Evening, Night)\n",
    "- `order_day` which will be the day of the week the order was made. (Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday)\n",
    "-  `order_month` which will be the month of the year the order was made. (January, February, March, April, May, June, July, August, September, October, November, December)\n",
    "- `order_season` which will be the season the order was made. (Summer, Autumn, Winter, Spring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ff07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_en(df_clean):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original one\n",
    "    df = df_clean.copy()\n",
    "\n",
    "   # Explode the 'order_items' column\n",
    "    df_exploded = df.explode('order_items')\n",
    "\n",
    "    # Split the tuple into two new columns\n",
    "    df_exploded[['cuisine', 'quantity_ordered']] = pd.DataFrame(df_exploded['order_items'].tolist(), index=df_exploded.index)\n",
    "\n",
    "    # average_order_price = order_items_total / quantity_ordered\n",
    "    df_exploded['average_item_price'] = df_exploded['order_price'] / df_exploded['quantity_ordered']\n",
    "\n",
    "    # two decimal places\n",
    "    df_exploded['average_item_price'] = df_exploded['average_item_price'].round(2)\n",
    "\n",
    "    # Drop the 'order_items' column\n",
    "    df_exploded.drop('order_items', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # Extract the year, month as Jan, Feb, Mar, etc. Add them as new columns\n",
    "    df_exploded['order_month'] = df_exploded['order_date'].dt.strftime('%b')\n",
    "    df_exploded['day_of_week'] = df_exploded['order_date'].dt.strftime('%a')\n",
    "\n",
    "    # new column for the season the order was made. (Spring, Summer, Autumn, Winter)\n",
    "    df_exploded['order_season'] = df_exploded['order_date'].dt.month.apply(lambda x: (x%12 + 3)//3)\n",
    "    \n",
    "    # change the season number to season name\n",
    "    df_exploded['order_season'] = df_exploded['order_season'].map({1:'Spring', 2:'Summer', 3:'Autumn', 4:'Winter'})\n",
    "\n",
    "    # reposition the columns \n",
    "    df_exploded = df_exploded[['order_id', 'order_date' , 'order_price', 'quantity_ordered' , 'average_item_price', 'order_month', 'day_of_week', 'order_season' , 'order_type', 'branch_code' , 'delivery_fee', 'location', 'cuisine' , 'customer_loyalty', 'distance_to_customer_km','updated_at']]\n",
    "\n",
    "    return df_exploded\n",
    "\n",
    "df_exploded = feature_en(df_clean)\n",
    "df_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2983b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4009a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eafb9790",
   "metadata": {},
   "source": [
    "### Step 2: Descriptive Statistics \n",
    "\n",
    "- Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset's distribution, excluding NaN values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebbddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statistics\n",
    "df_exploded.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d5a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base on the information from the describe() method:\n",
    "\n",
    "#  Orders Date Range \n",
    "print('Orders Date Range ---->: ', df_exploded['order_date'].min(), 'to', df_exploded['order_date'].max())\n",
    "\n",
    "# Order Price Range\n",
    "print('Order Price Range ---->: ', df_exploded['order_price'].min(), 'to', df_exploded['order_price'].max())\n",
    "\n",
    "# Average Order Price in 2 decimal places\n",
    "print('Average Order Price ---->: ', round(df_exploded['order_price'].mean(), 2))\n",
    "\n",
    "# Popular Order Type\n",
    "print('Popular Order Type ---->: ', df_exploded['order_type'].mode()[0])\n",
    "\n",
    "# Popular Cuisine\n",
    "print('Popular Cuisine ---->: ', df_exploded['cuisine'].mode()[0])\n",
    "\n",
    "# Popular Branch\n",
    "print('Popular Branch ---->: ', df_exploded['branch_code'].mode()[0])\n",
    "\n",
    "# Popular Day of the Week\n",
    "print('Popular Day of the Week ---->: ', df_exploded['day_of_week'].mode()[0])\n",
    "\n",
    "# Popular Season\n",
    "print('Popular Season ---->: ', df_exploded['order_season'].mode()[0])\n",
    "\n",
    "# Popular Month\n",
    "print('Popular Month ---->: ', df_exploded['order_month'].mode()[0])\n",
    "\n",
    "# Pupluar Hour\n",
    "print('Popular Hour ---->: ', df_exploded['order_date'].dt.hour.mode()[0])\n",
    "\n",
    "# Delivery Fee Range\n",
    "print('Delivery Fee Range ---->: ', df_exploded['delivery_fee'].min(), 'to', df_exploded['delivery_fee'].max())\n",
    "\n",
    "# Average Disance to Customer in 2 decimal places\n",
    "print('Average Disance to Customer ---->: ', round(df_exploded['distance_to_customer_km'].mean(), 2))\n",
    "\n",
    "# Popular Location\n",
    "print('Popular Location ---->: ', df_exploded['location'].mode()[0])\n",
    "\n",
    "# Types of cuisine\n",
    "print('Types of cuisine ---->: ', df_exploded['cuisine'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040323ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b98b1a3",
   "metadata": {},
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
